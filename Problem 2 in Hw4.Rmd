---
title: "Problem 2"
author: "Xinyi Song"
date: "10/14/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 2: Using the dual nature to our advantage

Sometimes using a mixture of true matrix math plus component operations cleans up our code giving better readibility.  Suppose we wanted to form the following computation:

\begin{itemize}
    \item $while(abs(\Theta_0^{i}-\Theta_0^{i-1}) \text{ AND } abs(\Theta_1^{i}-\Theta_1^{i-1}) > tolerance) \text{ \{ }$
    \begin{eqnarray*}
        \Theta_0^i &=& \Theta_0^{i-1} - \alpha\frac{1}{m}\sum_{i=1}^{m} (h_0(x_i) -y_i)  \\
        \Theta_1^i &=& \Theta_1^{i-1} - \alpha\frac{1}{m}\sum_{i=1}^{m} ((h_0(x_i) -y_i)x_i) 
    \end{eqnarray*}
    $\text{ \} }$
\end{itemize}

Where $h_0(x) = \Theta_0 + \Theta_1x$.  

Given $\mathbf{X}$ and $\vec{h}$ below, implement the above algorithm and compare the results with lm(h~0+$\mathbf{X}$).  State the tolerance used and the step size, $\alpha$.

```{r}
set.seed(1256)
theta <- as.matrix(c(1,2),nrow=2)
X <- cbind(1,rep(1:10,10))
h <- as.vector(X%*%theta+rnorm(100,0,0.2))
m <- dim(X)[1]
#theta = matrix(0,2,1)
THETA = matrix(5,2,1)
alpha = 0.01
while((abs(theta[1] - THETA[1])>1e-06) || (abs(theta[2] - THETA[2])>1e-06)){
  THETA = theta
  h_0 = X%*%THETA
  h_y = sweep(as.matrix(h_0), 1, h, '-')
  theta[1] = THETA[1] - alpha*mean(h_y)
  h_yx = sweep(h_y, 1, as.matrix(X[,2]), '*')
  theta[2] = THETA[2] - alpha*mean(h_yx)
}
print(theta)
# regression 
dat = as.data.frame(cbind(h,X))
fit = lm(h~0+X, data = dat)
summary(fit)
```
Here, I use alpha - the step size as 0.001 and tolerance value as 0.001, the results are very closed.


## Problem 3

The above algorithm is called Gradient Descent.  This algorithm, like Newton's method, has "hyperparameters" that are determined outside the algorithm and there are no set rules for determing what settings to use.  For gradient descent, you need to set a start value, a step size and tolerance.  

### Part a. Using a step size of $1e^{-7}$ and tolerance of $1e^{-9}$, try 10000 different combinations of start values for $\beta_0$ and $\beta_1$ across the range of possible $\beta$'s +/-1 from true determined in Problem 2, making sure to take advantages of parallel computing opportunities.  In my try at this, I found starting close to true took 1.1M iterations, so set a stopping rule for 5M.  Report the min and max number of iterations along with the starting values for those cases.  Also report the average and stdev obtained across all 10000 $\beta$'s.

```{r}
set.seed(1256)
X <- cbind(1,rep(1:10,10))
theta <- as.matrix(c(1,2),nrow=2)
h <- as.vector(X%*%theta+rnorm(100,0,0.2))
THETAs = expand.grid(seq(0, 2, length.out = 100), seq(1, 3, length.out = 100))
# function of Gradient Descent
# Modify the function to do parallel programming
grad = function(theta_start, X, h){
  theta_old_0 = 1000
  theta_old_1 = 1000
  alpha = 1e-2 # step size 
  tol = 1e-05 # tolerance value
  theta_new_0 = theta_start[1]
  theta_new_1 = theta_start[2]
  i = 0 # i: iteration times 
  while((abs(theta_new_0-theta_old_0)>tol) || (abs(theta_new_1 - theta_old_1)>tol)){
    theta_old_0 = theta_new_0
    theta_old_1 =  theta_new_1
    theta_h = rbind(theta_old_0, theta_old_1)
    h_0 = X%*%theta_h
    h_y = sweep(as.matrix(h_0), 1, h, '-')
    theta_new_0 = theta_old_0 - alpha*mean(h_y)
    h_yx = sweep(h_y, 1, as.matrix(X[,2]), '*')
    theta_new_1 = theta_old_1 - alpha*mean(h_yx)
    i = i + 1
    if(i>5000000) break
  }
  result = c(i, theta_new_0, theta_new_1)
  return(result)
}
```


```{r}
library(parallel)
# A good number of clusters is the numbers of available cores minus 1.
no_cores <- detectCores() - 1
cl = makeCluster(no_cores) # not work
#cl <- parallel::makeCluster(no_cores, setup_strategy = "sequential") # work
clusterExport(cl, 'X')
clusterExport(cl, 'h')
start_time <- Sys.time()
# Here, I only try two observations
a <- parApply(cl,THETAs[1:2,], 1, grad, X, h)
stopCluster(cl)
end_time <- Sys.time()
end_time - start_time
min_iteration = min(a[1,])
max_iteration = max(a[1,])
mean_theta_0 = mean(a[2,])
sd_theta_0 = sqrt(var(a[2,]))
mean_theta_1 = mean(a[3,])
sd_theta_1 = sqrt(var(a[3,]))
iteration_summary = cbind(min_iteration, max_iteration)
print(iteration_summary)
theta_summary = cbind(mean_theta_0, mean_theta_1, sd_theta_0, sd_theta_1)
print(theta_summary)
```